{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5728283b-57d5-4fb5-8bcd-2bd28434b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e5b96f-01f3-4d39-a657-3006b0e6ec5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>commenttext</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ded get call cthulhu weird tale vintage classi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love lovecraft agree call cthulhu not best wor...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>call cthulhu never best work simply popular we...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shadow over innsmouth second popular work righ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  sentiment\n",
       "0                                        commenttext          2\n",
       "1  ded get call cthulhu weird tale vintage classi...          3\n",
       "2  love lovecraft agree call cthulhu not best wor...          3\n",
       "3  call cthulhu never best work simply popular we...          2\n",
       "4  shadow over innsmouth second popular work righ...          2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "books_data = pd.read_csv(r\"C:\\Users\\wu02x\\Downloads\\SC4021\\new_pre_processed_data.csv\")\n",
    "books_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29101fcf-5e86-4361-ad05-6cc71003f0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    13906\n",
       "2     9258\n",
       "3     6448\n",
       "0     4354\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9d78f0-cdf8-4961-ad07-20eefd91fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data = books_data[(books_data['sentiment'] == 0) | (books_data['sentiment'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d59f66-f8de-4310-9b54-885eabef22d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    4354\n",
       "0    4354\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downsample the data due to impbalance in positive and negative data\n",
    "positive_class_samples = books_data[books_data['sentiment'] == 1].sample(n=len(books_data[books_data['sentiment'] == 0]), random_state=42)\n",
    "negative_class_samples = books_data[books_data['sentiment'] == 0]\n",
    "# Concatenate minority and majority class samples\n",
    "books_data = pd.concat([positive_class_samples, negative_class_samples])\n",
    "\n",
    "# Shuffle the dataset\n",
    "books_data = books_data.sample(frac=1, random_state=42)\n",
    "books_data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6f50e5-2bd0-47b2-beee-d5ec8ce792a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the 80% data for training data and 20% for testing data and maintain equal ratio of classes in the train and test sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(books_data['comment_text'], books_data['sentiment'], test_size=0.2, stratify=books_data['sentiment'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33c06c2f-9677-45b7-b305-49192c6a2785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of each review :  42.25861276986679\n"
     ]
    }
   ],
   "source": [
    "s = 0.0\n",
    "for i in books_data ['comment_text']:\n",
    "    word_list = i.split()\n",
    "    s = s + len(word_list)\n",
    "print(\"Average length of each review : \",s/books_data .shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dede3d9-845b-42d8-a486-5fcbef830357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 26685\n"
     ]
    }
   ],
   "source": [
    "# Find vocab size\n",
    "# Concatenate all the text in the 'comment_text' column into a single string\n",
    "all_text = ' '.join(books_data ['comment_text'])\n",
    "\n",
    "# Tokenize the string into individual words\n",
    "words = all_text.split()\n",
    "\n",
    "# Create a set from the tokenized words to remove duplicates\n",
    "vocab = set(words)\n",
    "\n",
    "# Find the length of the set, which will give you the vocabulary size\n",
    "vocab_size = len(vocab)\n",
    "#\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3d978ba-088e-4ada-947a-89c77fa27d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters of the model\n",
    "vocab_size = 28000\n",
    "oov_tok = ''\n",
    "embedding_dim = 50\n",
    "max_length = 80\n",
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length)\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33fcbc73-8cc9-4be8-b583-7e06e69ee10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wu02x\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:89: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wu02x\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 781ms/step - accuracy: 0.5684 - loss: 0.6758 - val_accuracy: 0.7135 - val_loss: 0.5729\n",
      "Epoch 2/100\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 800ms/step - accuracy: 0.8024 - loss: 0.4467 - val_accuracy: 0.7400 - val_loss: 0.5260\n",
      "Epoch 3/100\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 781ms/step - accuracy: 0.8902 - loss: 0.2805 - val_accuracy: 0.7440 - val_loss: 0.5952\n",
      "Epoch 4/100\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 747ms/step - accuracy: 0.9370 - loss: 0.1745 - val_accuracy: 0.7382 - val_loss: 0.7457\n",
      "Epoch 5/100\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 787ms/step - accuracy: 0.9605 - loss: 0.1128 - val_accuracy: 0.7273 - val_loss: 0.8752\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Define parameter\n",
    "n_lstm = 128\n",
    "drop_lstm = 0.2\n",
    "# Define LSTM Model \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_shape=(max_length,)))\n",
    "model.add(SpatialDropout1D(drop_lstm))\n",
    "model.add(LSTM(n_lstm, return_sequences=False))\n",
    "model.add(Dropout(drop_lstm))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Fit model with early stopping\n",
    "history = model.fit(train_padded, y_train,\n",
    "                    epochs=num_epochs, verbose=1,\n",
    "                    validation_data=(test_padded, y_test),\n",
    "                    callbacks=[early_stopping]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98815e1e-165a-4205-8a1d-a18a94ffcbfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 307ms/step\n",
      "F1 score: 0.732\n",
      "Precision score: 0.720\n",
      "Recall score: 0.745\n",
      "Accuracy score: 0.727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, average_precision_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "result = model.predict(test_padded)\n",
    "\n",
    "# For example, you might round probabilities to the nearest integer\n",
    "y_pred_binary = [1 if p > 0.5 else 0 for p in result]\n",
    "\n",
    "# Compute F1 score\n",
    "F1_score = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# Calculate average precision\n",
    "average_precision = average_precision_score(y_test, y_pred_binary)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "\n",
    "# Calculate recall\n",
    "recall_score = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "print('F1 score: {0:0.3f}'.format(F1_score))\n",
    "print('Precision score: {0:0.3f}'.format(precision))\n",
    "print('Recall score: {0:0.3f}'.format(recall_score))\n",
    "# print('Average precision-recall score: {0:0.3f}'.format(average_precision))\n",
    "print('Accuracy score: {0:0.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e8daa-0de4-4b78-8373-e35a414ce847",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57c02777-0949-477e-b389-217adf941c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataset\n",
    "eval_filepath = r\"C:\\Users\\wu02x\\Downloads\\SC4021\\evaluation_preprocessed_data.csv\"\n",
    "eval_data = pd.read_csv(eval_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be855188-3ce6-4db4-93da-3b4e6648f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column from float to int\n",
    "eval_data['manual_label'] = eval_data['manual_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "732f956c-4f12-4f90-8aa9-445c24dabf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = eval_data.comment_text\n",
    "y_eval = eval_data.manual_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32af4461-a01e-4d03-a1a0-5160c0674913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Test dataset to sequence and pad sequences\n",
    "eval_sequences = tokenizer.texts_to_sequences(X_eval)\n",
    "eval_padded = pad_sequences(eval_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3c2b1ab-1c69-4bfd-b5ff-4b13b9a930f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 306ms/step\n",
      "Classification Time for 1000 records: 7.780988454818726 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4610    0.8500    0.5978       160\n",
      "           1     0.9503    0.7427    0.8338       618\n",
      "\n",
      "    accuracy                         0.7648       778\n",
      "   macro avg     0.7057    0.7964    0.7158       778\n",
      "weighted avg     0.8497    0.7648    0.7853       778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Make predictions on the evaluation set\n",
    "result = model.predict(eval_padded)\n",
    "\n",
    "y_pred_binary = [1 if p > 0.5 else 0 for p in result]\n",
    "\n",
    "end_time = time.time()\n",
    "classification_time = end_time - start_time\n",
    "\n",
    "print(\"Classification Time for 1000 records:\", classification_time, \"seconds\")\n",
    "\n",
    "print(classification_report(y_eval, y_pred_binary, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10c10692-1625-4a90-9e1f-9ff391042fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wu02x\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 314ms/step\n",
      "F1 score: 0.834\n",
      "Precision score: 0.950\n",
      "Recall score: 0.743\n",
      "Accuracy score: 0.765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, average_precision_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "result = model.predict(eval_padded)\n",
    "\n",
    "# For example, you might round probabilities to the nearest integer\n",
    "y_pred_binary = [1 if p > 0.5 else 0 for p in result]\n",
    "\n",
    "# Compute F1 score\n",
    "F1_score = f1_score(y_eval, y_pred_binary)\n",
    "\n",
    "# Calculate average precision\n",
    "average_precision = average_precision_score(y_eval, y_pred_binary)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_eval, y_pred_binary)\n",
    "\n",
    "# Calculate recall\n",
    "recall_score = recall_score(y_eval, y_pred_binary)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_eval, y_pred_binary)\n",
    "\n",
    "print('F1 score: {0:0.3f}'.format(F1_score))\n",
    "print('Precision score: {0:0.3f}'.format(precision))\n",
    "print('Recall score: {0:0.3f}'.format(recall_score))\n",
    "# print('Average precision-recall score: {0:0.3f}'.format(average_precision))\n",
    "print('Accuracy score: {0:0.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43daeb-1611-4eff-969b-2b917c00dea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
